# CMU SCS 15-799 | Query Cost Models & Statistics

## Motivation

- The DBMS uses a **cost model** to estimate the physical execution cost of a query plan given a particular database state.
- These costs are purely _internal_ (not necessarily equal to actual wall-clock time).
- The cost model is fed by **statistics** that describe the contents of the database (or relevant portions of it) in a summarized form.

### Key Observations
1. We cannot feasibly run all possible plans to see which is faster.
2. A cost model needs to rely on **statistics** (e.g., histograms, sketches, etc.) to approximate:
   - How many tuples an operator will process.
   - How many tuples an operator will emit.
3. Physical operator costs depend on resource weighting factors (e.g., CPU vs. I/O cost).

---

## Two Main Cost Model Components

1. **Physical Costs**
   - CPU cycles, disk I/O, memory usage, network overhead, etc.
   - Tied to the specific physical implementation of each operator (e.g., hash join vs. merge join).
   - Relies on “magic” constants or calibration to reflect hardware characteristics.

2. **Logical Costs**
   - Input and output cardinalities of each operator (i.e., row-count estimates).
   - Not tied to the actual algorithm (hash join vs. merge join); it only cares about how many rows come in and out.
   - Also sometimes includes tuple width (e.g., how many bytes per tuple).

### Approximation Layers
- We approximate the data distribution via **statistics** (first approximation).
- Then we approximate how an operator’s predicates filter data (second approximation).
- If we chain operators, the errors compound further.

---

## Database Statistics

A DBMS typically stores summarized metadata about each table (and each column) in its **internal catalog**:

- **Single-Column Statistics**: By default, most systems track stats individually per column.
- **Column-Group / Multi-Column Statistics**: Optionally, the DBA or the system can track correlations among multiple columns (e.g., city/zip, or columns used in the same index).
- **Triggering Mechanisms** to Refresh Stats:
  - Periodic (e.g., nightly maintenance)
  - Threshold-based (e.g., if 20% of the table changes, re-run `ANALYZE`)
  - Manual invocation
  - Certain sophisticated DBMSs update stats automatically more frequently.

### Example: PostgreSQL Stats
- Stored in catalog tables like `pg_stats`, `pg_class`, etc.
- Histograms or “most common values / frequencies” are array fields in these internal tables.
- By default, `ANALYZE` populates these. Without it, PG will have no stats for newly loaded data.

### Example: MySQL, MSSQL, Oracle
- Similar ideas: each stores stats in an internal schema. Sometimes combined with auto-scheduling (e.g., Oracle 10 PM auto tasks, or MSSQL auto update stats).

---

## Summaries for Selectivity Estimation

Four major categories:

1. **Histograms**
   - Classic approach: bucket the domain of possible values and store frequency counts.
   - Many variations:
     - **Equi-Width**: each bucket spans the same range width.
     - **Equi-Depth (Quantiles)**: each bucket has roughly the same total count.
     - **End-Biased**: store exact frequencies for the most common values, then one “overflow” bucket for the rest.
2. **Sketches**
   - Probabilistic data structures (e.g., **Count-Min Sketch**, **HyperLogLog**, **t-digest**).
   - Typically used to approximate:
     - Frequent items (count-min),
     - Number of distinct elements (HyperLogLog),
     - Quantiles (t-digest), etc.
   - Fast merges, low memory overhead, but narrower usage than histograms.
3. **Sampling**
   - The DBMS can keep a small random sample (or sample in real-time) and run the predicate on it to approximate selectivity. 
   - Great in principle, but typically more overhead and rarely done in mainstream systems (some use it in special cases).
4. **Machine Learning Models** (Experimental)
   - Train a learned model to predict cardinalities. 
   - Complexity is high, usage is not widespread in production.

---

## Histograms

### Equi-Width Histograms
- Subdivide the attribute’s value range into equal “widths” (e.g., 1–10, 11–20, 21–30, ...).
- Each bucket stores the count of how many data values fall into that range.
- Problem: can be inaccurate if distribution is skewed (some buckets might contain many more tuples than others).

### Equi-Depth Histograms (Quantiles)
- Subdivide so that each bucket has roughly the same number of data points.
- Generally lower error in worst-case or average-case scenarios vs. equi-width.

### End-Biased / Most Common Values
- Keep exact frequency for the top K frequent values.
- All remaining values get lumped into one “average” bucket.

---

## Sketches

Probabilistic structures used to store frequency/count approximations with bounded error:

### Count-Min Sketch
- Maintains a 2D array of counters with multiple hash functions.
- Insertion: hash the key once per “row” in the array, increment that cell.
- Query (e.g., “frequency of key x”): compute the same hashes for x, read each relevant cell, take their **minimum**. 
- Merging: sum the counters cell-by-cell if the parameters and hash functions align.

### HyperLogLog
- Used to approximate **count distinct**.
- Splits hashed keys into “b bits for bucket index, rest bits for trailing zero count.” 
- Each bucket captures how far you must scan until you see the first 1-bit.
- The cardinality is derived using the harmonic mean of these bucket counts plus a corrective factor.
- Merging: take the **max** across the corresponding registers.

---

## Sampling

- The system keeps a small subset of each table (often ~1% or smaller).
- For a given predicate, the optimizer might run a quick test on the sample to see how many sample rows satisfy the filter => approximate global selectivity.
- Rarely done in most DBMSs automatically. SQL Server uses small samples at times, others do not by default.

---

## Real-World Cost Model Implementations

### PostgreSQL
- Assumes a disk-based environment with little memory.
- Has “magic” constants:
  - `cpu_tuple_cost`, `cpu_index_tuple_cost`, `random_page_cost`, `seq_page_cost`...
- Default: CPU is 400× cheaper than disk I/O, sequential I/O is 4× cheaper than random I/O.
- *DBA can tweak them, but it is guesswork unless you systematically measure them yourself.*

### IBM Db2
- Runs microbenchmarks at start-up or at intervals to figure out hardware cost parameters.
- Tracks concurrency environment, memory sizes, average # of users, etc.

### Smallbase (TimesTen)
- Two-phase approach:
  1. Identify all “execution primitives” (very fine-grained operators).
  2. Run microbenchmarks to see how many CPU cycles each primitive costs.
  3. Then the total operator cost = (# times each primitive used) × (cost from step 2).

### DuckDB
- Often sees data it has never seen before (e.g., CSV files, Parquet files).
- If no stats, it makes default assumptions:
  - If a HyperLogLog is available, use that for a better guess.
  - Otherwise, assume about 20% selectivity for many unknown filters, plus other simplified heuristics (uniform distribution, PK–FK joins, etc.).

---

## Closing Thoughts

- The DBMS’s cost model is built on **approximations**:
  - Summaries of data distributions (histograms, sketches, sampling).
  - Error-laden cardinality estimations feed into operator-level cost calculations.
  - Weighted physical resource costs (CPU vs. I/O) with guessed or micro-benchmarked factors.
- Errors can compound, but **some** form of cost model is still vital.
- Future directions / existing research: 
  - **Adaptivity** (where we correct errors at runtime),
  - **Machine Learning** for advanced modeling,
  - **More sophisticated multi-column or multi-table correlations**,
  - **In-situ** or ephemeral data handling (data lakes).