# CMU SCS 15-799 | Query Cost Models: Cardinality Estimation

## 1. Introduction

This lecture continues our discussion on how **query optimizers** use cost models to estimate the amount of data that each operator will process or produce. We specifically focus on *cardinality estimation*, which predicts the number of rows that qualify for filters or appear in join outputs. Accurate cardinalities are crucial; if the optimizer underestimates or overestimates too much, it will likely choose a suboptimal plan (e.g., picking a nested-loop join when a hash join is actually cheaper).

The lecture also references the empirical findings from the so-called “**JOB**” (Join Order Benchmark) on real data (IMDB), showing how multiple major DBMSs systematically misestimate cardinalities—especially with multi-join queries.

---

## 2. Recap & Motivation

From the previous lecture, we know:

- **Statistics** (histograms, sketches) help approximate table contents.
- **Cost models** combine these statistics with formulas for CPU/I/O to estimate operator costs.
- The single biggest source of error in a typical DBMS optimizer is **cardinality estimation** (predicting the number of tuples after filters/joins).

Hence, the class is moving from discussing data structures for stats (histograms, HLL, etc.) to how these stats get used to calculate cardinalities. We’ll then see how inaccurate estimates can cause severe plan regressions.

Key Points:

- **Accurate cardinalities** are generally more important than a fancy CPU/disk weighting model.
- Every major system makes certain simplifying assumptions (uniform distribution, independence of predicates, etc.), which break down in real data.

---

## 3. Cardinality Estimation Basics

**Cardinality Estimation** is how the optimizer predicts the number of rows output by each operator (scan, filter, join, etc.). These estimates propagate upward:

1. *Single Predicate*: `age = 9`  
   - The system uses a histogram (or similar) to find how many rows match `age=9` relative to the total.
2. *Range Predicate*: `age >= 7`  
   - The system sums up histogram buckets above 7, plus a fractional piece for the bucket containing 7.
3. *Negation*: `age != 2`  
   - Typically: `sel(not P) = 1 – sel(P)`.

### Uniformity & Independence

- **Uniformity**: Within a histogram bucket (e.g., `[6..8]`), values are assumed equally frequent.  
- **Independence**: If multiple predicates exist on a table, the DBMS often multiplies their selectivities. Example: `sel(A=2 AND name LIKE 'A%') = sel(A=2) × sel(name LIKE 'A%')`. This can severely underestimate or overestimate if attributes are correlated.

### Example of Correlation

For a database of cars:
```sql
SELECT *
  FROM cars
 WHERE make = 'Honda' 
   AND model = 'Accord';
```
- Independence assumption: `(1/10) * (1/100) = 0.001`  
- Reality: Only Honda makes Accord => Real selectivity = `1/100 = 0.01`  
- Off by factor of 10.

---

## 4. Multiple Selection Conditions

When a query has two or more predicates on the same table:

- **Conjunction** (`AND`):  
  \[
    \text{sel}(P1 \wedge P2) \approx \text{sel}(P1) \times \text{sel}(P2)
  \]
- **Disjunction** (`OR`):  
  \[
    \text{sel}(P1 \vee P2) \approx \text{sel}(P1) + \text{sel}(P2) - (\text{sel}(P1) \times \text{sel}(P2))
  \]

Again, the DBMS is typically using the **independence** assumption. Some systems do small adjustments to avoid major errors, but it’s still a big source of mistakes if the columns are correlated.

---

## 5. Join Size Estimation

Beyond single-table predicates, the optimizer must estimate the size of **joins**. The common formula often used:

```
|R ⋈ S|  ≈  (|R| * |S|) / max(V(A,R), V(A,S))
```
where `A` is the join attribute, and `V(A,R)` is the number of distinct values of `A` in `R`.

### Containment Principle

Systems often assume each key in the inner table is guaranteed to appear in the outer, ensuring the joined domain is not “uncovered.” This can be inaccurate if some keys only appear in one table.

### Error Propagation

Once the system gets a wrong cardinality at the first join, every subsequent join or filter in the plan can be miscalculated. With multi-join queries (e.g., 5 or 6+ tables), cardinality estimates tend to degrade rapidly.

---

## 6. Empirical Observations (JOB Benchmark)

A well-known paper, *“How Good Are Query Optimizers, Really?” (VLDB 2015)*, by Leis et al. (the “Germans”), tested many commercial and open-source DBMSs with the **JOB** workload on IMDB data. They discovered:

- Systems systematically underestimate cardinalities, especially for 6+ joins.
- SQL Server was somewhat better, while others (Postgres, MySQL, etc.) were worse.
- Large errors in cardinality cause suboptimal plans (e.g., picking nested-loop join instead of a hash join).

The overarching conclusion: **cardinality errors matter more** than whether we do fancy CPU vs. I/O weighting in the cost model.

---

## 7. Implementations

### 7.1 PostgreSQL

- Maintains an internal cost model that sums up:
  - *Sequential page cost* vs. *random page cost*
  - *CPU cost* for each row
- The user or DBA must tune the constants (e.g., `seq_page_cost`, `random_page_cost`) to match hardware. By default, Postgres assumes spinning disks with high random access cost. 
- The cardinality formulas themselves are fairly textbook-based, ignoring correlation unless the user creates manual extended statistics.

**Experiments** show that if Postgres drastically underestimates cardinalities, it chooses nested-loop joins that become very slow at runtime.

### 7.2 Microsoft SQL Server

- Has “row goals” to guide whether we need just the first few rows or the entire set.
- Distinguishes cost to produce the first tuple vs. all tuples (affects choosing index nested-loop vs. scan).
- **Multiple predicates**: applies an *exponential backoff* heuristic. Sort predicates from most selective to least, then multiply:

  \[
  \text{Estimate} = |R| \times S_1 \times \sqrt{S_2} \times \sqrt{\sqrt{S_3}} \cdots
  \]
  
  This is meant to avoid severe underestimations when columns are correlated.

---

## 8. Conclusion

**Takeaways**:

1. **Cardinality estimation** is the hardest problem in query optimization.  
2. Traditional assumptions (uniformity, independence, containment) cause big estimation errors on real data.  
3. Research suggests that **accurate cardinalities** matter more than a sophisticated cost model.  
4. Some vendors (SQL Server) have special heuristics (exponential backoff, sampling, advanced multi-column stats) to reduce errors.  
5. Future approaches: **machine learning** or adaptive query execution can further address cardinality mistakes.